<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>1st Monocular Depth Estimation Challenge</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="1st Monocular Depth Estimation Challenge" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MDEC @ WACV 2023" />
<meta property="og:description" content="MDEC @ WACV 2023" />
<link rel="canonical" href="http://localhost:4000/wacv2023.html" />
<meta property="og:url" content="http://localhost:4000/wacv2023.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="1st Monocular Depth Estimation Challenge" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"MDEC @ WACV 2023","headline":"1st Monocular Depth Estimation Challenge","url":"http://localhost:4000/wacv2023.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<link rel="apple-touch-icon" sizes="180x180" href="assets/imgs/favicon/apple-touch-icon.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="assets/imgs/favicon/favicon-32x32.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="assets/imgs/favicon/favicon-16x16.jpg">
<link rel="manifest" href="assets/imgs/favicon/site.webmanifest">

<!--Set all links to open in new tab by default-->
<base target="_blank">

<!-- end custom head snippets -->


  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">1<sup>st</sup> Monocular Depth Estimation Challenge</h1>
      <h2 class="project-tagline">MDEC @ WACV 2023</h2>
<!--      -->
      

      <a href="#news" class="btn" target="_self">News</a>
      <a href="#dates" class="btn" target="_self">Important Dates</a>
      <a href="#schedule" class="btn" target="_self">Schedule</a>
      <a href="#speakers" class="btn" target="_self">Keynote Speakers</a>
      <a href="#winners" class="btn" target="_self">Challenge Winners</a>
      <a href="#challenge" class="btn" target="_self">Challenge</a>
      <a href="#organizers" class="btn" target="_self">Organizers</a>
      <br>
      <a href="/" class="btn" target="_self">Latest</a>
      <a href="" class="btn" target="_self">1. CVPR 2022</a>
      <a href="" class="btn" target="_self">2. CVPR 2021</a>
    </header>

    <script src="assets/js/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop({
      diameter: 56,
      backgroundColor: '#4196ff',
      textColor: '#fff',
    })</script>

    <main id="content" class="main-content" role="main">
      <p class="text-center"><img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> Welcome to the <strong>1<sup>st</sup> Monocular Depth Estimation Challenge Workshop</strong> organized at <img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> <a href="https://wacv2023.thecvf.com"><img class="rounded-rect" src="assets/imgs/wacv2023.png" width="400px" alt="wacv2023"></a></p>

<div class="container">
<img class="img-syns" src="assets/imgs/syns/image_0026.png" alt="image_0026">
<img class="img-syns" src="assets/imgs/syns/image_0254.png" alt="image_0254">
<img class="img-syns" src="assets/imgs/syns/image_0698.png" alt="image_0698">

<img class="img-syns" src="assets/imgs/syns/depth_0026.png" alt="depth_0026">
<img class="img-syns" src="assets/imgs/syns/depth_0254.png" alt="depth_0254">
<img class="img-syns" src="assets/imgs/syns/depth_0698.png" alt="depth_0698">
</div>

<p>Monocular depth estimation (<strong>MDE</strong>) is an important low-level vision task, with application in fields such as augmented reality, robotics and autonomous vehicles.
Recently, there has been an increased interest in <strong>self-supervised systems</strong> capable of predicting the <strong>3D scene structure</strong> without requiring ground-truth LiDAR training data.
Automotive data has accelerated the development of these systems, thanks to the vast quantities of data, the ubiquity of stereo camera rigs and the mostly-static world.
However, the evaluation process has also remained focused on only the automotive domain and has been largely unchanged since its inception, relying on simple metrics and sparse LiDAR data.</p>

<p>This workshop seeks to answer the following questions:</p>
<ol>
  <li>How well do networks generalize beyond their training distribution relative to humans?</li>
  <li>What metrics provide the most insight into the model’s performance? 
What is the relative weight of simple cues, e.g. height in the image, in networks and humans?</li>
  <li>How do the predictions made by the models differ from how humans perceive depth? 
Are the failure modes the same?</li>
</ol>

<p>The workshop will therefore consist of two parts: 
invited <a href="#speakers" target="_self">keynote talks</a> discussing current developments in MDE 
and a <a href="#challenge" target="_self">challenge</a> organized around a novel <a href="https://arxiv.org/abs/2208.01489"><strong>benchmarking procedure</strong></a> 
using the <a href="https://www.nature.com/articles/srep35805"><strong>SYNS dataset</strong></a>.</p>

<h2 id="page_facing_up-paper">
<img class="emoji" title=":page_facing_up:" alt=":page_facing_up:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png" height="20" width="20"> <strong>Paper</strong>
</h2>
<p><a href="https://arxiv.org/abs/2211.12174"><img src="assets/imgs/paper_wacv2023.png" alt="paper"></a></p>

<h2 id="tv-videos">
<img class="emoji" title=":tv:" alt=":tv:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4fa.png" height="20" width="20"> <strong>Videos</strong>
</h2>

<!--From https://github.com/nathancy/jekyll-embed-video-->

<div class="embed-container">
  <iframe src="https://www.youtube.com/embed/QgP81xwdpik" width="380" height="240" allowfullscreen="true">
  </iframe>
</div>

<!--From https://github.com/nathancy/jekyll-embed-video-->

<div class="embed-container">
  <iframe src="https://www.youtube.com/embed/RJSWZssGtAQ" width="380" height="240" allowfullscreen="true">
  </iframe>
</div>

<!--From https://github.com/nathancy/jekyll-embed-video-->

<div class="embed-container">
  <iframe src="https://www.youtube.com/embed/4_SlDLgU_a8" width="380" height="240" allowfullscreen="true">
  </iframe>
</div>

<!--From https://github.com/nathancy/jekyll-embed-video-->

<div class="embed-container">
  <iframe src="https://www.youtube.com/embed/ORV05MuElfg" width="380" height="240" allowfullscreen="true">
  </iframe>
</div>

<!--From https://github.com/nathancy/jekyll-embed-video-->

<div class="embed-container">
  <iframe src="https://www.youtube.com/embed/5NvGVHZX7ME" width="380" height="240" allowfullscreen="true">
  </iframe>
</div>

<h2 id="news">
<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> <strong>News</strong>
</h2>
<ul>
  <li>
<strong>07 Jan 2023 —</strong> The workshop has now concluded! Talks will be made available soon.</li>
  <li>
<strong>19 Dec 2022 —</strong> <img class="emoji" title=":calendar:" alt=":calendar:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c6.png" height="20" width="20"> Workshop schedule released.</li>
  <li>
<strong>23 Nov 2022 —</strong> <img class="emoji" title=":page_facing_up:" alt=":page_facing_up:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png" height="20" width="20"> The paper summarizing the challenge is now available on <a href="https://arxiv.org/abs/2211.12174">ArXiv</a>.</li>
  <li>
<strong>14 Nov 2022 —</strong> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> The challenge has now finished! Thank you to all participants.</li>
  <li>
<strong>04 Nov 2022 —</strong> <img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> Challenge submission deadline has been extended until 14-Nov-2022.</li>
  <li>
<strong>26 Oct 2022 —</strong> <img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> Final phase of the challenge is live.</li>
  <li>
<strong>30 Sep 2022 —</strong> <img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> Challenge website is live! Running from 05-Oct-2022 to 08-Nov-2022.</li>
  <li>
<strong>16 Sep 2022 —</strong> <img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> <strong>Oisin Mac Aodha</strong> confirmed as keynote speaker.</li>
  <li>
<strong>17 Aug 2022 —</strong> <img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> <strong>James Elder</strong> confirmed as keynote speaker.</li>
  <li>
<strong>17 Aug 2022 —</strong> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Website is live!</li>
</ul>

<hr>

<h2 id="dates">
<img class="emoji" title=":hourglass_flowing_sand:" alt=":hourglass_flowing_sand:" src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png" height="20" width="20"> <strong>Important Dates</strong>
</h2>
<ul>
  <li>
<strong>05 Oct 2022 (00:00 UTC) —</strong> Challenge Development Phase <strong>Opens</strong> (Val)</li>
  <li>
<strong>26 Oct 2022 (00:00 UTC) —</strong> Challenge Final Phase <strong>Opens</strong> (Test)</li>
  <li>
<strong>08 Nov 2022 (23:59 UTC) —</strong> Challenge Submission <strong>Closes</strong>
</li>
  <li>
<strong>14 Nov 2022 (23:59 UTC) —</strong> Challenge Submission <strong>Closes</strong> (UPDATED)</li>
  <li>
<strong>11 Nov 2022 —</strong> Method Description Submission</li>
  <li>
<strong>15 Nov 2022 —</strong> Invited Talk Notification</li>
  <li>
<strong>07 Jan 2023 (Half-day AM) —</strong> MDEC Workshop @ WACV 2023</li>
</ul>

<hr>

<h2 id="schedule">
<img class="emoji" title=":calendar:" alt=":calendar:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c6.png" height="20" width="20"> <strong>Schedule</strong>
</h2>

<blockquote>
  <p><strong>NOTE</strong>: Times are shown in <strong>Hawaii Standard Time</strong>. 
Please take this into account if joining the workshop virtually.</p>

  <p><strong>*Virtual talk only.</strong>
<strong>All other talks will be hybrid.</strong></p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Time (HST)</th>
      <th>Duration</th>
      <th>Event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>08:20 - 08:30</td>
      <td>10 mins</td>
      <td>Introduction to Workshop</td>
    </tr>
    <tr>
      <td>08:30 - 09:15</td>
      <td>45 mins</td>
      <td>
<strong>Oisin Mac Aodha</strong> – Advancing Monocular Depth Estimation*</td>
    </tr>
    <tr>
      <td>09:15 - 10:00</td>
      <td>45 mins</td>
      <td>
<strong>James Elder</strong> – Monocular 3D Perception in Humans and Machines</td>
    </tr>
    <tr>
      <td>10:00 - 10:30</td>
      <td>30 mins</td>
      <td><em>Break</em></td>
    </tr>
    <tr>
      <td>10:30 - 11:00</td>
      <td>30 mins</td>
      <td>The Monocular Depth Estimation Challenge</td>
    </tr>
    <tr>
      <td>11:00 - 11:20</td>
      <td>20 mins</td>
      <td>Challenge Participant: <strong>Team z.suri</strong>*</td>
    </tr>
    <tr>
      <td>11:20 - 11:40</td>
      <td>20 mins</td>
      <td>Challenge Participant: <strong>Team MonoViT</strong>*</td>
    </tr>
    <tr>
      <td>11:40 - 11:50</td>
      <td>10 mins</td>
      <td>Closing Notes</td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="speakers">
<img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> <strong>Keynote Speakers</strong>
</h2>

<div class="container">
<figure>
    <a href="https://www.elderlab.yorku.ca/jelder/">
    <img class="img-author" src="assets/imgs/authors/james_elder.jpg" alt="James Elder"></a>
    <b><br><a href="https://www.elderlab.yorku.ca/jelder/">James Elder</a>
    <br>Professor <br>York University</b>
</figure>

<figure>
    <a href="https://homepages.inf.ed.ac.uk/omacaod/">
    <img class="img-author" src="assets/imgs/authors/oisin_macaodha.jpeg" alt="Oisin Mac Aodha"></a>
    <b><br><a href="https://homepages.inf.ed.ac.uk/omacaod/">Oisin Mac Aodha</a>
    <br>Assistant Professor <br>University of Edinburgh</b>
</figure>
</div>

<p><a href="https://www.elderlab.yorku.ca/jelder/"><strong>James Elder</strong></a>
is Professor and York Research Chair in Human and Computer Vision, Department of Electrical Engineering &amp; Computer Science (Lassonde School of Engineering), Department of Psychology (Faculty of Health) and Co-Director of the Centre for AI &amp; Society at York University, Toronto, Canada.
Dr. Elder’s research seeks to improve machine vision systems through a better understanding of visual processing in biological systems.
He currently leads the ORF-RE project <em>Intelligent Systems for Sustainable Urban Mobility</em>.
He also holds a number of patents on attentive vision technologies and is the co-founder of the AI start-up <em>AttentiveVision</em>.
He is appointed to the Editorial Boards of three international journals.</p>

<p><a href="https://homepages.inf.ed.ac.uk/omacaod/"><strong>Oisin Mac Aodha</strong></a>
is a Lecturer in Machine Learning in the School of Informatics at the University of Edinburgh.
From 2016-2019, he was a postdoc in Prof. Pietro Perona’s Computational Vision Lab at Caltech.
Prior to that, he was a postdoc in the Department of Computer Science at University College of London (UCL) with Prof. Gabriel Brostow and Prof. Kate Jones.
He received his PhD from UCL in 2014, advised by Prof. Gabriel Brostow, and has an MSc in Machine Learning from UCL an BEng in electronic and computing engineering from the University of Galway.
Along with being a Fellow of the Alan Turing Institute and a European Laboratory for Learning and Intelligent Systems (ELLIS) Scholar.
His current research interests are in the areas of computer vision and machine learning, with a specific emphasis on shape and depth estimation, human-in-the-loop learning, and fine-grained image understanding.</p>

<hr>

<h2 id="winners">
<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> <strong>Challenge Winners</strong>
</h2>

<p>Congratulations to the <strong>OPDAI</strong> team on achieving the top performing submission (F-Score)!</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>F-Score</th>
      <th>F-Score<br>(Edges)</th>
      <th>MEA</th>
      <th>RMSE</th>
      <th>AbsRel</th>
      <th>Acc<br>(Edges)</th>
      <th>Comp<br>(Edges)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Baseline</strong></td>
      <td><span style="color:green"><strong>13.72</strong></span></td>
      <td><span style="color:green"><strong>7.76</strong></span></td>
      <td>5.56</td>
      <td>9.72</td>
      <td>32.04</td>
      <td>3.97</td>
      <td><span style="color:green"><strong>21.63</strong></span></td>
    </tr>
    <tr>
      <td><strong>OPDAI</strong></td>
      <td><span style="color:blue;text-decoration: underline">13.53</span></td>
      <td>7.41</td>
      <td><span style="color:green"><strong>5.20</strong></span></td>
      <td><span style="color:blue;text-decoration: underline">8.98</span></td>
      <td><span style="color:green"><strong>29.66 </strong></span></td>
      <td><span style="color:blue;text-decoration: underline">3.67</span></td>
      <td><span style="color:blue;text-decoration: underline">27.31</span></td>
    </tr>
    <tr>
      <td><strong>z.suri</strong></td>
      <td>13.08</td>
      <td>7.46</td>
      <td>5.39</td>
      <td>9.27</td>
      <td>29.96</td>
      <td>3.81</td>
      <td>32.70</td>
    </tr>
    <tr>
      <td><strong>Anonymous</strong></td>
      <td>12.85</td>
      <td>7.30</td>
      <td>5.32</td>
      <td>9.04</td>
      <td>30.22</td>
      <td>3.83</td>
      <td>43.77</td>
    </tr>
    <tr>
      <td><strong>MonoViT</strong></td>
      <td>12.66</td>
      <td><span style="color:blue;text-decoration: underline">7.51</span></td>
      <td><span style="color:blue;text-decoration: underline">5.22</span></td>
      <td><span style="color:green"><strong>8.96</strong></span></td>
      <td><span style="color:blue;text-decoration: underline">29.70</span></td>
      <td><span style="color:green"><strong>3.36</strong></span></td>
      <td>35.47</td>
    </tr>
  </tbody>
</table>

<h3 id="teams"><strong>Teams</strong></h3>
<ul>
  <li>
<strong>OPDAI</strong>: Hao Wang, Yusheng Zhang, Heng Cong</li>
  <li>
<strong>z.suri</strong>: Zeeshan Khan Suri</li>
  <li><strong>Anonymous</strong></li>
  <li>
<strong>MonoViT</strong>: Chaoqiang Zhao, Mateo Poggi, Fabio Tosi, Youming Zhang, Yang Tang, Stefano Mattoccia</li>
</ul>

<hr>

<h2 id="challenge">
<img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> <strong>Challenge</strong>
</h2>
<p><strong>Teams submitting to the challenge will also be required to submit a description of their method.
As part of the WACV Proceedings, we will publish a paper summarizing the results of the challenge, including a description of each method.
All challenge participants surpassing the performance of the Garg baseline [13.7211] (by jspenmar) will be added as authors in this paper.
Top performers will additionally be invited to present their method <a href="#schedule" target="_self">at the workshop</a>.
This presentation can be either in-person or virtually.</strong></p>

<p class="text-center"><strong>[<a href="https://github.com/jspenmar/monodepth_benchmark">GitHub</a>] — [<a href="https://codalab.lisn.upsaclay.fr/competitions/7811">Challenge</a>] — [<a href="https://arxiv.org/abs/2208.01489">Paper</a>]</strong></p>

<p>The challenge focuses on evaluating novel MDE techniques on the <strong>SYNS-Patches</strong> dataset proposed in <a href="https://arxiv.org/abs/2208.01489"><strong>this benchmark</strong></a>.
This dataset provides a challenging variety of urban and natural scenes, including forests, agricultural settings, residential streets, industrial estates, lecture theatres, offices and more.
Furthermore, the high-quality dense ground-truth LiDAR allows for the computation of more informative evaluation metrics, such as those focused on <a href="https://arxiv.org/abs/1805.01328v1"><strong>depth discontinuities</strong></a>.</p>

<div class="container">
<img class="img-syns" src="assets/imgs/syns/image_0551.png" alt="image_0551">
<img class="img-syns" src="assets/imgs/syns/image_0893.png" alt="image_0893">
<img class="img-syns" src="assets/imgs/syns/image_1114.png" alt="image_1114">

<img class="img-syns" src="assets/imgs/syns/depth_0551.png" alt="depth_0551">
<img class="img-syns" src="assets/imgs/syns/depth_0893.png" alt="depth_0893">
<img class="img-syns" src="assets/imgs/syns/depth_1114.png" alt="depth_1114">
</div>

<p>The challenge is hosted on <a href="https://codalab.lisn.upsaclay.fr/competitions/7811"><strong>CodaLab</strong></a>. 
We have provided a <a href="https://github.com/jspenmar/monodepth_benchmark"><strong>GitHub repository</strong></a> containing training and evaluation code for multiple recent SotA approaches to MDE.
These will serve as a competitive baseline for the challenge and as a starting point for participants.
The challenge leaderboards use the withheld validation and test sets for <strong>SYNS-Patches</strong>.
We additionally encourage evaluation on the public <a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction"><strong>Kitti Eigen-Benchmark</strong></a> dataset.</p>

<p>Submissions will be evaluated on a variety of metrics:</p>
<ol>
  <li>
<a href="https://arxiv.org/abs/2203.08122"><strong>Pointcloud reconstruction</strong></a>: F-Score</li>
  <li>
<a href="https://arxiv.org/abs/1708.06500"><strong>Image-based depth</strong></a>: MAE, RMSE, AbsRel</li>
  <li>
<a href="https://arxiv.org/abs/1805.01328v1"><strong>Depth discontinuities</strong></a>: F-Score, Accuracy, Completeness</li>
</ol>

<p>Challenge winners will be determined based on the <strong>pointcloud-based F-Score</strong> performance.</p>

<hr>

<h2 id="organizers">
<img class="emoji" title=":construction_worker:" alt=":construction_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f477.png" height="20" width="20"> <strong>Organizers</strong>
</h2>
<div class="container">
<figure>
    <a href="https://www.surrey.ac.uk/people/jaime-spencer-martin">
    <img class="img-author" src="assets/imgs/authors/jaime_spencer.jpg" alt="Jaime Spencer"></a>
    <b><br><a href="https://www.surrey.ac.uk/people/jaime-spencer-martin">Jaime Spencer</a>
    <br>Research Fellow <br>University of Surrey</b>
</figure>

<figure>
    <a href="https://research.aston.ac.uk/en/persons/stella-qian">
    <img class="img-author" src="assets/imgs/authors/stella_qian.png" alt="Stella Quian"></a>
    <b><br><a href="https://research.aston.ac.uk/en/persons/stella-qian">Stella Qian</a>
    <br>Research Fellow <br>Aston University</b>
</figure>

<figure>
    <a href="https://www.amazon.science/author/chris-russell?0000016e-4318-de2e-a76e-cfdfa9700000-page=2">
    <img class="img-author" src="assets/imgs/authors/chris_russell.jpeg" alt="Chris Russell"></a>
    <b><br><a href="https://www.amazon.science/author/chris-russell?0000016e-4318-de2e-a76e-cfdfa9700000-page=2">Chris Russell</a>
    <br>Senior Applied Scientist <br>Amazon</b>
</figure>

<figure>
    <a href="http://personal.ee.surrey.ac.uk/Personal/S.Hadfield/biography.html">
    <img class="img-author" src="assets/imgs/authors/simon_hadfield.png" alt="Simon Hadfield"></a>
    <b><br><a href="http://personal.ee.surrey.ac.uk/Personal/S.Hadfield/biography.html">Simon Hadfield</a>
    <br>Senior Lecturer <br>University of Surrey</b>
</figure>

<figure>
    <a href="https://www.southampton.ac.uk/people/5wzxpy/doctor-erich-graf">
    <img class="img-author" src="assets/imgs/authors/erich_graf.jpeg" alt="Erich Graf"></a>
    <b><br><a href="https://www.southampton.ac.uk/people/5wzxpy/doctor-erich-graf">Erich Graf</a>
    <br>Associate Professor <br>University of Southampton</b>
</figure>

<figure>
    <a href="https://research.aston.ac.uk/en/persons/andrew-schofield">
    <img class="img-author" src="assets/imgs/authors/andrew_schofield.png" alt="Andrew Schofield"></a>
    <b><br><a href="https://research.aston.ac.uk/en/persons/andrew-schofield">Andrew Schofield</a>
    <br>Professor <br>Aston University</b>
</figure>

<figure>
    <a href="http://personal.ee.surrey.ac.uk/Personal/R.Bowden/">
    <img class="img-author" src="assets/imgs/authors/richard_bowden.png" alt="Richard Bowden"></a>
    <b><br><a href="http://personal.ee.surrey.ac.uk/Personal/R.Bowden/">Richard Bowden</a>
    <br>Professor <br>University of Surrey</b>
</figure>
</div>


      <footer class="site-footer">
        <span class="site-footer-owner">Maintained by <a href="https://github.com/sayands">sayands</a></span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
