<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Computer Vision in the Built Environment</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Computer Vision in the Built Environment" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CV4AEC @ CVPR 2023" />
<meta property="og:description" content="CV4AEC @ CVPR 2023" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Computer Vision in the Built Environment" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"CV4AEC @ CVPR 2023","headline":"Computer Vision in the Built Environment","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<link rel="apple-touch-icon" sizes="180x180" href="assets/imgs/favicon/apple-touch-icon.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="assets/imgs/favicon/favicon-32x32.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="assets/imgs/favicon/favicon-16x16.jpg">
<link rel="manifest" href="assets/imgs/favicon/site.webmanifest">

<!--Set all links to open in new tab by default-->
<base target="_blank">

<!-- end custom head snippets -->


  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Computer Vision in the Built Environment</h1>
      <h2 class="project-tagline">CV4AEC @ CVPR 2023</h2>
<!--      -->
      

      <a href="#news" class="btn" target="_self">News</a>
      <a href="#dates" class="btn" target="_self">Important Dates</a>
      <a href="#schedule" class="btn" target="_self">Schedule</a>
      <a href="#speakers" class="btn" target="_self">Keynote Speakers</a>
      <a href="#winners" class="btn" target="_self">Challenge Winners</a>
      <a href="#challenge" class="btn" target="_self">Challenge</a>
      <a href="#organizers" class="btn" target="_self">Organizers</a>
      <br>
      <a href="/" class="btn" target="_self">Latest</a>
      <a href="" class="btn" target="_self">1. CVPR 2022</a>
      <a href="" class="btn" target="_self">2. CVPR 2021</a>
    </header>

    <script src="assets/js/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop({
      diameter: 56,
      backgroundColor: '#4196ff',
      textColor: '#fff',
    })</script>

    <main id="content" class="main-content" role="main">
      <p class="text-center"><img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> Welcome to the <strong>3<sup>rd</sup> Workshop and Challenge on
Computer Vision In The Built Environment For The Design, Construction and Operation of Buildings</strong> organized at <img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> <a href="https://cvpr2023.thecvf.com/"><img class="rounded-rect" src="assets/imgs/cvpr2023.svg" width="400px" alt="cvpr2023"></a></p>

<p>Building on the success of the previous two workshops, the 3rd Workshop on Computer Vision in the Built Environment continues on connecting the domains of Architecture, Engineering, and Construction (AEC) with that of Computer Vision by establishing a common ground of interaction and identify shared research interests. Specifically, this workshop focuses on the as-is semantic status of built environments and the changes that take place within them over time. These topics will be presented from the dual lens of Computer Vision and AEC-FM, highlighting the limitations and bottlenecks related to developing applications for this specific domain. The objective is for attendees to learn more about AEC-FM and the variety of real-world problems that, if solved, could have a tangible impact on this multi trillion dollar industry as well as the overall quality of life across the globe.</p>

<p>The workshop will begin by establishing ways to capture the as-is status of a space with expert speakers from both the AEC and Computer Vision domains. Attendees will be then introduced to the type of information required for the spatiotemporal analysis of our built environment in AEC, with a focus on effective management, safety, and the role of users in this process. Following that, the topic of scene understanding from 3D and 4D reconstructions will be presented. Finally, to close the loop from understanding real-world built environments to designing built environments better and faster, the topic of scene synthesis at a geometric and semantic level will be presented. The importance of closing the loop for the AEC industry is paramount, especially when considering the design paradox. Architects are designing living spaces without any feedback from their previous designs. Learning to design using data from spaces that are already occupied and in-use, can provide designers with insights on what makes spaces appropriate for supporting the quality of life of the users.</p>

<p>To further establish connections between the two domains and identify what we can do right now and what is still hard to solve, we will host the <strong>3rd International Scan-to-BIM competition</strong> targeted on acquiring the semantic as-is status of buildings given their 3D point clouds. Specifically, we will focus on the tasks of floorplan reconstruction and 3D building model reconstruction and present appropriate interdisciplinary metrics for solving them. The past two years we observed that a large gap remains before these problems can be considered solved and actually meet the needs of practitioners. We regard this workshop as the ideal environment for understanding the challenges and steps forward given that it provides convergence between the research and practical communities from multiple disciplines.</p>

<!-- ## :newspaper: **News** {#news}
- **07 Jan 2023 ---** The workshop has now concluded! Talks will be made available soon. 
- **19 Dec 2022 ---** :calendar: Workshop schedule released.
- **23 Nov 2022 ---** :page_facing_up: The paper summarizing the challenge is now available on [ArXiv](https://arxiv.org/abs/2211.12174).
- **14 Nov 2022 ---** :trophy: The challenge has now finished! Thank you to all participants.
- **04 Nov 2022 ---** :checkered_flag: Challenge submission deadline has been extended until 14-Nov-2022.
- **26 Oct 2022 ---** :checkered_flag: Final phase of the challenge is live.
- **30 Sep 2022 ---** :checkered_flag: Challenge website is live! Running from 05-Oct-2022 to 08-Nov-2022.
- **16 Sep 2022 ---** :microphone: **Oisin Mac Aodha** confirmed as keynote speaker.
- **17 Aug 2022 ---** :microphone: **James Elder** confirmed as keynote speaker.
- **17 Aug 2022 ---** :tada: Website is live! -->

<hr>

<h2 id="dates">
<img class="emoji" title=":hourglass_flowing_sand:" alt=":hourglass_flowing_sand:" src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png" height="20" width="20"> <strong>Important Dates</strong>
</h2>
<blockquote>
  <p><strong>NOTE</strong>: The submission/release times are <strong>00:00:00 UTC</strong> on the specified date.</p>
</blockquote>

<ul>
  <li>
<strong>12 Apr 2023 —</strong> Training + Validation + Testing data available for 2D</li>
  <li>
<strong>14 Apr 2023 —</strong> Training + Testing data available for 3D</li>
  <li>
<strong>15 Apr 2023 —</strong> Evaluation server <strong>open</strong> to evaluate test submissions</li>
  <li>
<strong>02 Jun 2023 —</strong> Challenge Submission Deadline</li>
  <li>
<strong>07 Jun 2023 —</strong> Notification To Participants</li>
  <li>
<strong>18 Jun 2023 —</strong> CV4AEC Workshop @ CVPR 2023</li>
</ul>

<h2 id="schedule">
<img class="emoji" title=":calendar:" alt=":calendar:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c6.png" height="20" width="20"> <strong>Schedule</strong>
</h2>

<blockquote>
  <p><strong>NOTE</strong>: Times are shown in <strong>Pacific Standard Time</strong>. 
Please take this into account if joining the workshop virtually.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Time (PST)</th>
      <th>Duration</th>
      <th>Event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>09:00 - 09:30</td>
      <td>30 mins</td>
      <td>Introduction to Workshop and Challenge</td>
    </tr>
    <tr>
      <td>09:30 - 10:00</td>
      <td>30 mins</td>
      <td>
<strong>Avideh Zakhor</strong> – TBD</td>
    </tr>
    <tr>
      <td>10:00 - 10:30</td>
      <td>30 mins</td>
      <td>
<strong>Pinbo Tang</strong> – Efficient Operations of Aging Civil Infrastructure</td>
    </tr>
    <tr>
      <td>10:30 - 10:45</td>
      <td>15 mins</td>
      <td><em>Coffee Break</em></td>
    </tr>
    <tr>
      <td>10:45 - 11:30</td>
      <td>45 mins</td>
      <td>Winner Presentations, 2D Floorplan Reconstruction</td>
    </tr>
    <tr>
      <td>11:30 - 12:00</td>
      <td>30 mins</td>
      <td>
<strong>Manmohan Chandraker</strong> - TBD</td>
    </tr>
    <tr>
      <td>12:00 - 12:30</td>
      <td>30 mins</td>
      <td>
<strong>Mani Golparvar Fard</strong> - TBD</td>
    </tr>
    <tr>
      <td>12:30 - 13:30</td>
      <td>60 mins</td>
      <td><em>Lunch Break</em></td>
    </tr>
    <tr>
      <td>13:30 - 14:15</td>
      <td>45 mins</td>
      <td>Winner Presentations, 3D Building Model Reconstruction</td>
    </tr>
    <tr>
      <td>14:15 - 15:00</td>
      <td>45 mins</td>
      <td><em>Community Engagement</em></td>
    </tr>
    <tr>
      <td>15:00 - 15:30</td>
      <td>30 mins</td>
      <td>
<strong>Despoina Paschalidou</strong> – Manipulating 3D Environments</td>
    </tr>
    <tr>
      <td>15:30 - 16:00</td>
      <td>30 mins</td>
      <td>
<strong>Fernanda Leite</strong> – TBD</td>
    </tr>
    <tr>
      <td>16:00 - 16:30</td>
      <td>30 mins</td>
      <td><em>Coffee Break</em></td>
    </tr>
    <tr>
      <td>16:30 - 17:00</td>
      <td>30 mins</td>
      <td>
<em>Keynote Talk</em> – TBD</td>
    </tr>
    <tr>
      <td>17:00 - 17:45</td>
      <td>45 mins</td>
      <td><em>Panel Discussion</em></td>
    </tr>
    <tr>
      <td>17:45 - 18:00</td>
      <td>15 mins</td>
      <td><em>Conclusion Remarks</em></td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="speakers">
<img class="emoji" title=":microphone:" alt=":microphone:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a4.png" height="20" width="20"> <strong>Keynote Speakers</strong>
</h2>

<div class="container">

<figure>
    <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/zakhor.html">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/avideh_zakhor.jpg" alt="Avideh Zakhor"> </a>
    <b><br><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/zakhor.html">Avideh Zakhor</a>
    <br>Professor, EE <br>UC Berkeley</b>
</figure>

<figure>
    <a href="https://www.cmu.edu/cee/people/faculty/tang.html">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/pingbo_tang.jpg" alt="Pingbo Tang"></a>
    <b><br><a href="https://www.cmu.edu/cee/people/faculty/tang.html">Pingbo Tang</a>
    <br>Professor, CEE <br>CMU</b>
</figure>

<figure>
    <a href="https://cseweb.ucsd.edu/~mkchandraker/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/manmohan_chandraker.jpg" alt="Manmohan Chandraker"></a>
    <b><br><a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
    <br>Professor, CSE <br>UC San Diego</b>
</figure>

<figure>
    <a href="https://cee.illinois.edu/directory/profile/mgolpar">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/mani_golparvar.jpg" alt="Mani Golpavar Fard"></a>
    <b><br><a href="https://cee.illinois.edu/directory/profile/mgolpar">Mani Golpavar Fard</a>
    <br>Professor, CEE <br>UIUC</b>
</figure>

<figure>
    <a href="https://paschalidoud.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/despoina_paschalidou.jpg" alt="Despoina Paschalidou"></a>
    <b><br><a href="https://paschalidoud.github.io/">Despoina Paschalidou</a>
    <br>Postdoctoral Researcher, CS <br>Stanford</b>
</figure>

<figure>
    <a href="https://www.caee.utexas.edu/people/faculty/faculty-directory/leite">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/fernanda_leite.jpg" alt="Fernanda Leite"></a>
    <b><br><a href="https://paschalidoud.github.io/">Fernanda Leite</a>
    <br>Professor, CAEE <br> UT Austin</b>
</figure>

</div>

<p><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/zakhor.html"><strong>Avideh Zakhor</strong></a>
is a Professor of electrical engineering and computer science at the University of California at Berkeley (Berkeley) where she holds the Qualcomm chair. Prof. Zakhor’s research is focused on 3D computer vision, reality capture for augmented and virtual reality, sensor fusion, and application of deep learning to signal, image and video processing. Dr. Zakhor is the recipient of multiple awards underscoring both her academic and professional qualifications including the winner of phase 1 and 2 of U.S. Department of Energy sponsored E-Robot project in 2021 and 2022. She is a fellow of IEEE and was chosen as the  SPIE Electronic Imaging Scientist of the Year  in 2018. Dr. Zakhor has spun off numerous startups from her lab at UC Berkeley. In 2005, she co-founded UrbanScan to commercialize software/hardware systems for rapid 3D, automated 3D modeling of cities, which became part of Google Earth product when UrbanScan was acquired by Google (Nasdaq;GOOG) in 2007. In 2015, she founded Indoor Reality  to develop 3D indoor mapping technologies for fast, automated, visual documentation, asset management, and energy audits of existing buildings. Indoor Reality was acquired by a large multi-billion dollar construction supplies company in 2019.</p>

<p><a href="https://www.cmu.edu/cee/people/faculty/tang.html"><strong>Pingbo Tang</strong></a>
is an Associate Professor in the Department of Civil and Environmental Engineering. He founded and is directing Spatiotemporal Workflows and Resilient Management Laboratory (SWARM Lab). He obtained his Bachelor’s Degree in Civil Engineering in 2002, and his Master’s Degree in Bridge Engineering in 2005, both from Tongji University, Shanghai, China. He obtained his Ph.D. from the group of Advanced Infrastructure Systems (AIS) at Carnegie Mellon University in 2009. Tang is an expert on civil infrastructure operations and human systems engineering for civil infrastructure operational safety. His research explores remote sensing, human systems engineering, and information modeling technology in support of the spatiotemporal analyses needed for the effective management of workspaces, constructed facilities, and civil infrastructure systems. His ongoing studies have been examining sensing and modeling methods for comprehending the Human-Cyber-Physical-Systems (H-CPS) in accelerated construction and infrastructure operations (e.g., airport operations, power plant operations, water treatment plant control). He has published more than 100 peer-reviewed articles in these areas. The National Science Foundation (NSF), Department of Energy (DOE), The National Aeronautics and Space Administration (NASA), and the industry have funded his research efforts.</p>

<p><a href="https://cseweb.ucsd.edu/~mkchandraker/"><strong>Manmohan Chandraker</strong></a>
is an Associate Professor at the CSE department of the University of California, San Diego. His research interests are in computer vision, machine learning and graphics-based vision, with applications to autonomous driving and augmented reality. His works have received the Marr Prize Honorable Mention for Best Paper at ICCV 2007, the 2009 CSE Dissertation Award for Best Thesis at UCSD, a PAMI special issue on best papers of CVPR 2011, the Best Paper Award at CVPR 2014, the 2018 NSF CAREER Award and the 2018 and 2019 Google Daydream Research Award. He serves as an Area Chair at CVPR, ICCV, ECCV, ICVGIP and AAAI.</p>

<p><a href="%22https://cee.illinois.edu/directory/profile/mgolpar%22"><strong>Mani Golparvar Fard</strong></a> is a Professor of Civil Engineering and Computer Science &amp; Technology Entrepreneurship. He is a Faculty Entrepreneurial Fellow, Excellence Faculty Fellow, and the director of the Real-time and Automated Monitoring and Control (RAAMAC) lab at the University of Illinois at Urbana-Champaign (UIUC). He received his Ph.D. degree in Civil Engineering and MS degree in Computer Science from UIUC in 2010, MASc in Civil Engineering from the University of British Columbia in 2006, and MS and BS in Civil Engineering from Iran University of Science and Technology in 2005 and 2002 respectively. Prior to joining the faculty at UIUC, he was an Assistant Professor in Civil Engrg at Virginia Tech. Dr. Golparvar has worked with many national and international construction companies and most extensively with Turner Construction. Dr. Golparvar-Fard has several patents and is currently involved with Reconstruct Inc., an early-stage technology company with $28 million in venture capital funding that was founded based on the outcomes of his ongoing research projects.</p>

<p><a href="%22https://paschalidoud.github.io/%22"><strong>Despoina Paschalidou</strong></a> is a PostDoc at Stanford University working with Prof. Leo Guibas at the Geometric Computation Group. Prior to this, she did her PhD at the Max Planck Institute for Intelligent Systems in Tubingen and the Computer Vision Lab in ETH Zurich, under the guidance of Prof. Andreas Geiger and Prof. Luc van Gool. She received her Diploma in Electrical and Computer Engineering from the Aristotle University of Thessaloniki, in 2015. Her research interests revolve around editable and interpretable representations of 3D objects and scenes. She spent 1 year working with Prof. Sanja Fidler at NVIDIA Research on developing interactive tools for content creation. Moreover, she spent 6 months at FAIR working with Prof. Andrea Vedaldi and David Novotny on unsupervised 3D reconstruction from video data.</p>

<p><a href="https://www.caee.utexas.edu/people/faculty/faculty-directory/leite"><strong>Fernanda Leite</strong></a> a Professor in the Cockrell School of Engineering at the University of Texas at Austin. She holds the John A. Focht Centennial Teaching Fellowship in Civil Engineering. She combines expertise in architectural engineering and computing in her modeling and built environment research. She is an Associate Editor for the journal Automation in Construction. Most of her work has been in building and infrastructure systems information modeling, visualization and collaboration technologies, and circular economy in the built environment. At UT-Austin, Dr. Leite teaches courses on Building Information Modeling, Project Management and Economics, Construction Safety, and Sustainable Systems Engineering.</p>

<hr>

<h2 id="winners">
<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> <strong>Challenge Winners</strong>
</h2>
<blockquote>
  <p><strong>TBA</strong></p>
</blockquote>

<hr>

<h2 id="challenge">
<img class="emoji" title=":checkered_flag:" alt=":checkered_flag:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c1.png" height="20" width="20"> <strong>Challenge</strong>
</h2>
<p>The workshop will host the 3rd International Scan-to-BIM challenge. The challenge will include the following tasks:</p>

<p>I. 2D Floorplan Reconstruction <br>
II. 3D Building Model Reconstruction</p>

<h3 id="2d-floor-plan-reconstruction">2D Floor Plan Reconstruction</h3>

<p>The 2D Floorplan Reconstruction challenge contains a total of 31 buildings with multiple floors each and dozens of rooms on each floor. Of which, 20 buildings are designated as the training set, with a total of 49 point clouds. The validation and testing sets contain 5.5 buildings with 21 point clouds each. For each model, there is an aligned point cloud in LAZ format. For the training and validation sets, a corresponding floorplan aligned with the coordinate system of the point cloud is also provided. The 2D challenge is hosted on <strong><a href="https://codalab.lisn.upsaclay.fr/competitions/12386">Codalab</a></strong>. We have provided a <strong><a href="https://github.com/cv4aec/2d-floorplan-eval">Github repository</a></strong> containing the evaluation code and metrics for floorplan reconstruction. The submission should be made in the same JSON format as in the provided ground truth. We include metrics to evaluate the reconstruction of the walls, doors, and columns, as well as floor area in 2D :</p>

<ol>
  <li>
    <p><strong>Geometric Metrics</strong> <br>
 a. <em>IoU</em> of each room (a room is defined as a completely separated area with walls and doors). <br>
 b. <em>Accuracy of endpoints</em> : Precision/Recall at 3 different thresholds: 5cm, 10cm and 20cm, as well as the F-measure at each threshold will be evaluated in the coordinate system of the point cloud. The provided endpoints will be matched with the Hungarian algorithm to the point cloud, and every point that is within a certain threshold will be determined as a match. <br>
 c. <em>Orientation</em> For each matched line between the ground truth, we will compute the cosine similarity metric between them as the normalized dot product. If a line is not matched with ground truth, the cosine metric will be zero. Finally, the metric will be averaged over all the ground truth lines.</p>
  </li>
  <li>
    <p><strong>Topological Metrics</strong> <br>
 a. <em><a href="https://ieeexplore.ieee.org/document/5539950">Warping error</a></em> : The warping error will first warp the predicted floorplan to the ground truth with a homotopic deformation, and then compute the pixels that cannot match after the deformation. <br>
 b. <strong><em>Betti number error</em></strong> : The Betti number error will compare the Betti numbers between the prediction and the ground truth and output the absolute value of the difference.</p>
  </li>
</ol>

<h3 id="3d-building-model-reconstruction">3D Building Model Reconstruction</h3>

<p>The training data consists of 18 floors from 10 buildings. For each model, there is an aligned point cloud in LAZ format. The 3D building coordinates for walls, columns and doors are presented in 3 separate JSON files. We focus on the reconstruction of walls, columns, and doors. The 3D challenge is hosted on <strong><a href="https://codalab.lisn.upsaclay.fr/competitions/12405">Codalab</a></strong>. We have provided a <strong><a href="https://github.com/cv4aec/3d-matching-eval">Github repository</a></strong> containing the evaluation code and metrics for building model reconstruction. The submission should be made in the same JSON format as in the provided ground truth. We evaluate the submissions on a variety of metrics :</p>

<ol>
  <li>
<strong>3D IoU</strong> of the 3D bounding box of each wall</li>
  <li>
<strong>Accuracy of the endpoints</strong> : Precision/Recall at 3 different thresholds: 5cm, 10cm and 20cm, as well as F-measure will be evaluated in the coordinate system of the point cloud. The provided endpoints will be matched with the Hungarian algorithm to the point cloud, and every point that is within a certain threshold will be determined as a match. We evaluate per each of the three semantic types (i.e., wall, column, door).</li>
</ol>

<blockquote>
  <p>We would like to note that ALL the submissions <strong>need to be constructed automatically</strong> . Manual reconstructions are against the spirit of this challenge and will not be allowed.</p>
</blockquote>

<h2 id="questions">
<img class="emoji" title=":question:" alt=":question:" src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png" height="20" width="20"> <strong>Questions</strong>
</h2>
<p>Contact the organisers at <strong><a href="mailto:cv4aec.3d@gmail.com">cv4aec.3d@gmail.com</a></strong></p>

<!-- **Teams submitting to the challenge will also be required to submit a description of their method.
As part of the WACV Proceedings, we will publish a paper summarizing the results of the challenge, including a description of each method.
All challenge participants surpassing the performance of the Garg baseline [13.7211] (by jspenmar) will be added as authors in this paper.
Top performers will additionally be invited to present their method <a href="#schedule" target="_self">at the workshop</a>.
This presentation can be either in-person or virtually.**

**[[GitHub](https://github.com/jspenmar/monodepth_benchmark)] --- [[Challenge](https://codalab.lisn.upsaclay.fr/competitions/7811)] --- [[Paper](https://arxiv.org/abs/2208.01489)]**
{: .text-center}

The challenge focuses on evaluating novel MDE techniques on the **SYNS-Patches** dataset proposed in [**this benchmark**](https://arxiv.org/abs/2208.01489).
This dataset provides a challenging variety of urban and natural scenes, including forests, agricultural settings, residential streets, industrial estates, lecture theatres, offices and more.
Furthermore, the high-quality dense ground-truth LiDAR allows for the computation of more informative evaluation metrics, such as those focused on [**depth discontinuities**](https://arxiv.org/abs/1805.01328v1).



The challenge is hosted on [**CodaLab**](https://codalab.lisn.upsaclay.fr/competitions/7811). 
We have provided a [**GitHub repository**](https://github.com/jspenmar/monodepth_benchmark) containing training and evaluation code for multiple recent SotA approaches to MDE.
These will serve as a competitive baseline for the challenge and as a starting point for participants.
The challenge leaderboards use the withheld validation and test sets for **SYNS-Patches**.
We additionally encourage evaluation on the public [**Kitti Eigen-Benchmark**](http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction) dataset.

Submissions will be evaluated on a variety of metrics:
1. [**Pointcloud reconstruction**](https://arxiv.org/abs/2203.08122): F-Score
2. [**Image-based depth**](https://arxiv.org/abs/1708.06500): MAE, RMSE, AbsRel
3. [**Depth discontinuities**](https://arxiv.org/abs/1805.01328v1): F-Score, Accuracy, Completeness

Challenge winners will be determined based on the **pointcloud-based F-Score** performance. -->

<hr>
<h1 id="organizers"><strong>Organizers</strong></h1>
<h2 id="senior-organizers">
<img class="emoji" title=":construction_worker:" alt=":construction_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f477.png" height="20" width="20"> <strong>Senior Organizers</strong>
</h2>
<div class="container">
<figure>
    <a href="https://ir0.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/iroarmeni.jpg" alt="Iro Armeni"></a>
    <b><br><a href="https://ir0.github.io/">Iro Armeni</a>
    <br>Postdoctoral Researcher, CS &amp; CEE  <br> ETHZ</b>
</figure>

<figure>
    <a href="https://www.linkedin.com/in/erzhuo-ezra-che-40888137/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/erzhuoche.jpeg" alt="Iro Armeni"></a>
    <b><br><a href="https://www.linkedin.com/in/erzhuo-ezra-che-40888137/">Erzhuo Che</a>
    <br>Assistant Professor (Senior Research), CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="https://web.stanford.edu/~fischer/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/martinfischer.jpg" alt="Martin Fischer"></a>
    <b><br><a href="https://web.stanford.edu/~fischer/">Martin Fischer</a>
    <br>Professor, CEE <br> Stanford</b>
</figure>

<figure>
    <a href="https://www.cs.sfu.ca/~furukawa/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/yasufurukawa.jpg" alt="Yasutaka Furukawa"></a>
    <b><br><a href="https://www.cs.sfu.ca/~furukawa/">Yasutaka Furukawa</a>
    <br>Associate Professor, CS <br> Simon Fraser</b>
</figure>

<figure>
    <a href="https://fcl.ethz.ch/people/Module-Lead/daniel-hall.html#:~:text=Dr%20Daniel%20Hall%20is%20co,Geomatic%20Engineering%20at%20ETH%20Z%C3%BCrich.">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/danielhall.jpeg" alt="Daniel Hall"></a>
    <b><br><a href="https://fcl.ethz.ch/people/Module-Lead/daniel-hall.html#:~:text=Dr%20Daniel%20Hall%20is%20co,Geomatic%20Engineering%20at%20ETH%20Z%C3%BCrich.">Daniel Hall</a>
    <br>Assistant Professor, CEE <br> ETHZ</b>
</figure>

<figure>
    <a href="https://research.engr.oregonstate.edu/geomatics/faculty-members">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/jaehoonjung.jpeg" alt="Jaehoon Jung"></a>
    <b><br><a href="https://research.engr.oregonstate.edu/geomatics/faculty-members">Jaehoon Jung</a>
    <br>Assistant Professor (Senior Research), CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="http://web.engr.oregonstate.edu/~lif/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/fuxinli.jpg" alt="Fuxin Li"></a>
    <b><br><a href="http://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>
    <br>Associate Professor, CS <br> Oregon State</b>
</figure>

<figure>
    <a href="https://directory.forestry.oregonstate.edu/people/olsen-michael">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/michaelolsen.jpg" alt="Michael Olsen"></a>
    <b><br><a href="https://directory.forestry.oregonstate.edu/people/olsen-michael">Michael Olsen</a>
    <br>Associate Professor, CEE <br> Oregon State</b>
</figure>

<figure>
    <a href="https://people.inf.ethz.ch/pomarc/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/marcpollefeys.jpeg" alt="Marc Pollefeys"></a>
    <b><br><a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
    <br>Professor, CS <br> ETHZ</b>
</figure>

<figure>
    <a href="https://cce.oregonstate.edu/turkan">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/yeldaturkan.jpg" alt="Yelda Turkan"></a>
    <b><br><a href="https://cce.oregonstate.edu/turkan">Yelda Turkan</a>
    <br>Assistant Professor, CEE <br> Oregon State</b>
</figure>

</div>

<h2 id="student-organizers">
<img class="emoji" title=":grimacing:" alt=":grimacing:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f62c.png" height="20" width="20"> <strong>Student Organizers</strong>
</h2>
<div class="container">
<figure>
    <a href="https://sayandebsarkar.com/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/sayandebsarkar.png" alt="Sayan Deb Sarkar"></a>
    <b><br><a href="https://sayandebsarkar.com/">Sayan Deb Sarkar</a>
    <br>MSc CS <br> ETHZ</b>
</figure>

<figure>
    <a href="https://antonskoltech.github.io/">
    <img class="img-author" src="assets/imgs/authors/cvpr2023/antonegorov.jpeg" alt="Anton Egorov"></a>
    <b><br><a href="https://antonskoltech.github.io/">Anton Egorov</a>
    <br>Research Assistance <br> Oregon State </b>
</figure>

</div>


      <footer class="site-footer">
        <span class="site-footer-owner">Maintained by <a href="https://github.com/sayands">sayands</a></span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
